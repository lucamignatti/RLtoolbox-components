{
  "packages": {
    "rltoolbox_components": {
      "version": "0.1.0",
      "components": [
        "networks.MLPADC.MLPADC",
        "environments.gymnasium.GymnasiumEnvironment",
        "algorithms.dppo.DPPO",
        "loggers.console.ConsoleLogger",
        "loggers.grapher.GraphLogger"
      ]
    }
  },
  "components": {
    "actor_critic": {
      "package": "rltoolbox_components",
      "type": "networks.MLPADC.MLPADC",
      "input_dim": 8,
      "output_dim": 4,
      "hidden_dims": [128, 128, 64],
      "v_min": -15.0,
      "v_max": 15.0,
      "num_atoms": 51,
      "advantage_calculation_method": "quantile_sampling"
    },
    "env": {
      "package": "rltoolbox_components",
      "type": "environments.gymnasium.GymnasiumEnvironment",
      "env_name": "LunarLander-v3"
    },
    "dppo": {
      "package": "rltoolbox_components",
      "type": "algorithms.dppo.DPPO",
      "actor_critic": "actor_critic",
      "learning_rate": 0.0003,
      "gamma": 0.99,
      "gae_lambda": 0.95,
      "clip_ratio": 0.2,
      "value_loss_coef": 0.5,
      "entropy_coef": 0.01,
      "num_epochs": 10,
      "batch_size": 256,
      "update_frequency": 2048,
      "max_grad_norm": 0.5,
      "v_min": -15.0,
      "v_max": 15.0,
      "num_atoms": 51,
      "use_adaptive_epsilon": true,
      "adaptive_epsilon_beta": 1.0,
      "epsilon_min": 0.05,
      "epsilon_max": 0.3,
      "use_confidence_weighting": true,
      "confidence_weight_type": "entropy",
      "confidence_weight_delta": 1e-6,
      "normalize_confidence_weights": false,
      "advantage_calculation_method": "quantile_sampling",
      "reward_norm_G_max": 10.0
    },
    "consolelogger": {
      "package": "rltoolbox_components",
      "type": "loggers.console.ConsoleLogger"
    },
    "graphlogger": {
      "package": "rltoolbox_components",
      "type": "loggers.grapher.GraphLogger"
    }
  },
  "hooks": {
    "training_start": [],
    "episode_reset": ["env"],
    "environment_step": ["env"],
    "action_selection": ["actor_critic"],
    "experience_storage": ["dppo"],
    "episode_end": ["dppo", "consolelogger", "graphlogger"],
    "training_end": ["graphlogger"]
  },
  "training": {
    "max_episodes": 5000,
    "max_steps_per_episode": 1000
  },
  "evaluation": {
    "enabled": true,
    "num_episodes": 10
  },
  "seed": 42
}
